{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6zn__LMq8e-"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from IPython.display import Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jBrBu5N_5Jp"
      },
      "outputs": [],
      "source": [
        "def print_markdown(text):\n",
        "  display(Markdown(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rObsSv49ha6i"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXcq8ZrZiLgd",
        "outputId": "135ece28-ef01-4e34-e3d0-5814aa430c12"
      },
      "outputs": [],
      "source": [
        "client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU0AiPs5iNKY"
      },
      "outputs": [],
      "source": [
        "prompt = \"Who is first man\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = [\n",
        "        {\n",
        "        \"role\" : \"user\",\n",
        "        \"content\" : prompt\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print_markdown(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US_xTirTiUGH"
      },
      "outputs": [],
      "source": [
        "def generate_response(user_query):\n",
        "  \"\"\"\n",
        "  Generate a response for any user query using OpenAI/Gemini's API with no hard coding of topics.\n",
        "  The model is expected to handle diverse topics dynamically.\n",
        "  \"\"\"\n",
        "  # System message that instructs the model to answer any query across various topic\n",
        "\n",
        "  system_message = (\n",
        "      \"Your are a helpful assistant capable of answering questions on a wide range of topic,\"\n",
        "      \"including programming, history science etc.,\"\n",
        "      \"provide clear concise and accurate answers\"\n",
        "  )\n",
        "\n",
        "  # User's Question\n",
        "  user_message = user_query\n",
        "\n",
        "  # Create the chat completion using openai api\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "        messages =  [\n",
        "\n",
        "         { \"role\" : \"user\" , \"content\" : user_message},\n",
        "         { \"role\" : \"system\" , \"content\" : system_message}\n",
        "        ],\n",
        "        model = \"gpt-3.5-turbo\"\n",
        "  )\n",
        "\n",
        "  return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IwgU6Le7mfHl"
      },
      "outputs": [],
      "source": [
        "prompt = \"give me code for bacward propogation\"\n",
        "print(f\"AI:{generate_response(prompt)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI2LsD6lmv1E"
      },
      "outputs": [],
      "source": [
        "def interactive_chat():\n",
        "  \"\"\"\n",
        "  Start an interactive chat session where the user can ask question.\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Welcome! you can ask any question.\")\n",
        "\n",
        "  while True:\n",
        "    # Get user input\n",
        "\n",
        "    user_input = input(\"You:\")\n",
        "\n",
        "    #Check if the user wants ti exit the convervation\n",
        "\n",
        "    if user_input.lower() in [\"exit\",\"quit\"]:\n",
        "      print(\"Existing the chat\")\n",
        "      break\n",
        "\n",
        "    # Get the response from the AI model\n",
        "\n",
        "    response = generate_response(user_input)\n",
        "\n",
        "    print(f\"AI : {response}\")\n",
        "    print(\"To end chat session type :exit or quit\")\n",
        "\n",
        "  interactive_chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
